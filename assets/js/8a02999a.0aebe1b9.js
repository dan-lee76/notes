"use strict";(self.webpackChunknottes=self.webpackChunknottes||[]).push([[5050],{3905:(e,t,n)=>{n.d(t,{Zo:()=>g,kt:()=>c});var a=n(67294);function l(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){l(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,l=function(e,t){if(null==e)return{};var n,a,l={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(l[n]=e[n]);return l}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(l[n]=e[n])}return l}var s=a.createContext({}),u=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},g=function(e){var t=u(e.components);return a.createElement(s.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,l=e.mdxType,i=e.originalType,s=e.parentName,g=o(e,["components","mdxType","originalType","parentName"]),m=u(n),c=l,d=m["".concat(s,".").concat(c)]||m[c]||p[c]||i;return n?a.createElement(d,r(r({ref:t},g),{},{components:n})):a.createElement(d,r({ref:t},g))}));function c(e,t){var n=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var i=n.length,r=new Array(i);r[0]=m;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:l,r[1]=o;for(var u=2;u<i;u++)r[u]=n[u];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},91796:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>o,toc:()=>u});var a=n(87462),l=(n(67294),n(3905));const i={},r="Modelling, representing, & generating language",o={unversionedId:"Year3/3074/03",id:"Year3/3074/03",title:"Modelling, representing, & generating language",description:"Outline and objectives",source:"@site/docs/Year3/3074/03.md",sourceDirName:"Year3/3074",slug:"/Year3/3074/03",permalink:"/Year3/3074/03",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"year3",previous:{title:"Biases in AI (1) - Algorithmic & model bias",permalink:"/Year3/3074/02"},next:{title:"Biases in AI(2)",permalink:"/Year3/3074/04"}},s={},u=[{value:"Outline and objectives",id:"outline-and-objectives",level:3},{value:"Language Modelling",id:"language-modelling",level:2},{value:"Probabilistic Language Models",id:"probabilistic-language-models",level:3},{value:"Maximum Likelihood Estimation and its problems",id:"maximum-likelihood-estimation-and-its-problems",level:3},{value:"Smoothing",id:"smoothing",level:3},{value:"Conclusion",id:"conclusion",level:3},{value:"Language Generation",id:"language-generation",level:2},{value:"What is NLG",id:"what-is-nlg",level:3},{value:"Multiple schools of NLG",id:"multiple-schools-of-nlg",level:3}],g={toc:u};function p(e){let{components:t,...n}=e;return(0,l.kt)("wrapper",(0,a.Z)({},g,n,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"modelling-representing--generating-language"},"Modelling, representing, & generating language"),(0,l.kt)("h3",{id:"outline-and-objectives"},"Outline and objectives"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Modelling and generation are big topics")),(0,l.kt)("p",null,"Language ",(0,l.kt)("strong",{parentName:"p"},"modelling")," - Building a probabilistic model of language.\nLanguage ",(0,l.kt)("strong",{parentName:"p"},"generation")," - Using templates and probabilistic models of language to create natural-sounding testing"),(0,l.kt)("h2",{id:"language-modelling"},"Language Modelling"),(0,l.kt)("h3",{id:"probabilistic-language-models"},"Probabilistic Language Models"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Core problem of language modelling"),(0,l.kt)("li",{parentName:"ul"},"Given a sequence of tokens, predict the next token")),(0,l.kt)("p",null,"Difficult to predict what word is next/accurate, is it 'I like feeding pigeons' or 'I like feeding bludgeons'. It is difficult to observe as what if that sentence doesn't appear in our data set."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Each word ",(0,l.kt)("inlineCode",{parentName:"li"},"i")," depends only on a ",(0,l.kt)("strong",{parentName:"li"},"restricted set of preceding words")),(0,l.kt)("li",{parentName:"ul"},"Assumption: Language is a ",(0,l.kt)("strong",{parentName:"li"},"stochastic"),", ",(0,l.kt)("strong",{parentName:"li"},"memoryless process"))),(0,l.kt)("p",null,"We only care about the probabilities of individual words"),(0,l.kt)("h3",{id:"maximum-likelihood-estimation-and-its-problems"},"Maximum Likelihood Estimation and its problems"),(0,l.kt)("p",null,"... is the Maximum Likelihood Estimate (MLE)"),(0,l.kt)("h3",{id:"smoothing"},"Smoothing"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"What if we apply our language model on a new sentence",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"One novel bigram would pull the probability to 0")))),(0,l.kt)("h3",{id:"conclusion"},"Conclusion"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Smallest unit of thought is the n-gram",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Unigrams let us work with single words"),(0,l.kt)("li",{parentName:"ul"},"Bigrams let us work with pairs of consecutive words"),(0,l.kt)("li",{parentName:"ul"},"Trigrams let us work with triples of consecutive words"))),(0,l.kt)("li",{parentName:"ul"},"Language models let us compare the likelihood of documents")),(0,l.kt)("h2",{id:"language-generation"},"Language Generation"),(0,l.kt)("h3",{id:"what-is-nlg"},"What is NLG"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"NLG is the subfield of ai and computational linguistics that focuses on computer sytems that can ",(0,l.kt)("strong",{parentName:"li"},"produce understandable texts")),(0,l.kt)("li",{parentName:"ul"},"...")),(0,l.kt)("h3",{id:"multiple-schools-of-nlg"},"Multiple schools of NLG"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Gap-filling systems - Simplest, no novelty"),(0,l.kt)("li",{parentName:"ul"},"Rule-based systems - Expanded gap-filling system"),(0,l.kt)("li",{parentName:"ul"},"Grammatical function - Complex template system, low novelty, medium error rate"),(0,l.kt)("li",{parentName:"ul"},"Dynamic generation - Abstract representation, fully dynamic, high novelty, high error rate")),(0,l.kt)("p",null,"NLP = Natural Language Understanding + Natural Language Generation"))}p.isMDXComponent=!0}}]);