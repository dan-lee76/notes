"use strict";(self.webpackChunknottes=self.webpackChunknottes||[]).push([[1595],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>m});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},u=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),d=c(a),m=r,g=d["".concat(l,".").concat(m)]||d[m]||p[m]||i;return a?n.createElement(g,o(o({ref:t},u),{},{components:a})):n.createElement(g,o({ref:t},u))}));function m(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var c=2;c<i;c++)o[c]=a[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},92738:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var n=a(87462),r=(a(67294),a(3905));const i={},o="Biases in AI (1) - Algorithmic & model bias",s={unversionedId:"Year3/3074/02",id:"Year3/3074/02",title:"Biases in AI (1) - Algorithmic & model bias",description:"Algorithmic Bias",source:"@site/docs/Year3/3074/02.md",sourceDirName:"Year3/3074",slug:"/Year3/3074/02",permalink:"/Year3/3074/02",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"year3",previous:{title:"Natural Language Processing",permalink:"/Year3/3074/01"},next:{title:"Modelling, representing, & generating language",permalink:"/Year3/3074/03"}},l={},c=[{value:"Algorithmic Bias",id:"algorithmic-bias",level:2},{value:"Racial bias in technology",id:"racial-bias-in-technology",level:3},{value:"Racial algorithmic bias",id:"racial-algorithmic-bias",level:3},{value:"Data corpora for generating responses",id:"data-corpora-for-generating-responses",level:3},{value:"Language understanding for generating responses",id:"language-understanding-for-generating-responses",level:3},{value:"Word Embeddings",id:"word-embeddings",level:2},{value:"What can be done about it",id:"what-can-be-done-about-it",level:3}],u={toc:c};function p(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"biases-in-ai-1---algorithmic--model-bias"},"Biases in AI (1) - Algorithmic & model bias"),(0,r.kt)("h2",{id:"algorithmic-bias"},"Algorithmic Bias"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Systemic errors in computer systems that lead to unfair outcomes or judgements")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Equality Act (2010): It is against the law to discriminate against someone because of any of these protected characteristics")),(0,r.kt)("h3",{id:"racial-bias-in-technology"},"Racial bias in technology"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"System behaviour")," (inc errors, faults etc) leading to unfair outcomes or judgements due to a persons race "),(0,r.kt)("li",{parentName:"ul"},"Due to biased associations and stereotypes"),(0,r.kt)("li",{parentName:"ul"},"Can be due to race-based correlations - targeted advertising. Such as post code lottery"),(0,r.kt)("li",{parentName:"ul"},"Can be result of biased training data, due to 'historical' reasons or flawed human labelling")),(0,r.kt)("h3",{id:"racial-algorithmic-bias"},"Racial algorithmic bias"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Can be about the physical dimension of race",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Darker skin tones not recognised by sensors"),(0,r.kt)("li",{parentName:"ul"},"CV 'object detection' found to mis-identify those with darker skin 10% more other than lighter skin"),(0,r.kt)("li",{parentName:"ul"},"Facial recognition for identity matching in laws enforcement")))),(0,r.kt)("p",null,"Facial recognition for gender classification less accurate for those with darker skin"),(0,r.kt)("p",null,"Google used to show ads for names but stopped due to the word 'arrest' appeared for black0dentifying first names (60%) than for white first names (48%)"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"LLM Bias")," - Generally, LLMs encode biases in societies, such as gender stereotypes"),(0,r.kt)("p",null,"Schlesinger suggest that instead of just deleting words, chatbots should handle sensitive topics like race, power and justice well. Three areas of chatbot architecture relevant for generating better responses"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Data corpora - focusing on the underlying data"),(0,r.kt)("li",{parentName:"ul"},"Language understanding"),(0,r.kt)("li",{parentName:"ul"},"Learning")),(0,r.kt)("h3",{id:"data-corpora-for-generating-responses"},"Data corpora for generating responses"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Rather than assume it can be avoided, anticipate 'race-talk' - explicitly collect and aggregate dialogs to train chatbots in more respectful race-talk"),(0,r.kt)("li",{parentName:"ul"},"Ensure diverse representation of cultural references across ethnicities"),(0,r.kt)("li",{parentName:"ul"},"Diversity is who build the database is more lively to lead a diverse database!")),(0,r.kt)("h3",{id:"language-understanding-for-generating-responses"},"Language understanding for generating responses"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"NLPs focus on probabilistic language models, based on which words(tokens) are likely to appear after one another, powerful way to generate valid statements.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Doesn't tell you if a statement is racist, sexist, untrue or otherwise problematic")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Need a growing capacity for meaning-in-context in NLP - the areas of semantics and pragmatics"))),(0,r.kt)("h2",{id:"word-embeddings"},"Word Embeddings"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Word embeddings represent text data as vectors",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Concepts such as similarity, association are computed by ",(0,r.kt)("em",{parentName:"li"},"co-occurrence")),(0,r.kt)("li",{parentName:"ul"},"In NLP, this is conflated with ",(0,r.kt)("em",{parentName:"li"},"meaning")),(0,r.kt)("li",{parentName:"ul"},"Words are considered to have similar semantic meaning if the vectors are close together"),(0,r.kt)("li",{parentName:"ul"},"Differences between vectors represent relationships between words")))),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Demonstrate that word embeddings contain biases in their geometry that reflect stereotypes present in broader society")),(0,r.kt)("p",null,"Example of quantitative social exercise - 100 years of text data to show how stereotypes toward women and ethnic minorities change over time"),(0,r.kt)("h3",{id:"what-can-be-done-about-it"},"What can be done about it"),(0,r.kt)("p",null,"Change starts with understanding the problems"),(0,r.kt)("p",null,"Approaches to mitigate bias:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Technical Solutions - debiasing algorithms, better training data"),(0,r.kt)("li",{parentName:"ul"},"Transparency and openness - Limitations of training data, classification algorithms"),(0,r.kt)("li",{parentName:"ul"},"Diversity and inclusion - who's involved in building the models etc")))}p.isMDXComponent=!0}}]);