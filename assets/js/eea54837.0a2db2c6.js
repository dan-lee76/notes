"use strict";(self.webpackChunknottes=self.webpackChunknottes||[]).push([[1191],{3905:(e,t,r)=>{r.d(t,{Zo:()=>p,kt:()=>d});var n=r(67294);function a(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function i(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function l(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?i(Object(r),!0).forEach((function(t){a(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function o(e,t){if(null==e)return{};var r,n,a=function(e,t){if(null==e)return{};var r,n,a={},i=Object.keys(e);for(n=0;n<i.length;n++)r=i[n],t.indexOf(r)>=0||(a[r]=e[r]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)r=i[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var s=n.createContext({}),u=function(e){var t=n.useContext(s),r=t;return e&&(r="function"==typeof e?e(t):l(l({},t),e)),r},p=function(e){var t=u(e.components);return n.createElement(s.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var r=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),c=u(r),d=a,g=c["".concat(s,".").concat(d)]||c[d]||m[d]||i;return r?n.createElement(g,l(l({ref:t},p),{},{components:r})):n.createElement(g,l({ref:t},p))}));function d(e,t){var r=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=r.length,l=new Array(i);l[0]=c;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:a,l[1]=o;for(var u=2;u<i;u++)l[u]=r[u];return n.createElement.apply(null,l)}return n.createElement.apply(null,r)}c.displayName="MDXCreateElement"},61310:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>m,frontMatter:()=>i,metadata:()=>o,toc:()=>u});var n=r(87462),a=(r(67294),r(3905));const i={},l="Text classification",o={unversionedId:"Year3/3074/07",id:"Year3/3074/07",title:"Text classification",description:"Classification as generalisation",source:"@site/docs/Year3/3074/07.md",sourceDirName:"Year3/3074",slug:"/Year3/3074/07",permalink:"/Year3/3074/07",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"year3",previous:{title:"The Dark Side of NLP",permalink:"/Year3/3074/06"},next:{title:"Conversational and Voice User Interfaces",permalink:"/Year3/3074/08"}},s={},u=[{value:"Classification as generalisation",id:"classification-as-generalisation",level:2},{value:"Arising Issues",id:"arising-issues",level:2},{value:"ML is generalisation",id:"ml-is-generalisation",level:2},{value:"Machine Learning Examples",id:"machine-learning-examples",level:3},{value:"ML Approaches",id:"ml-approaches",level:2},{value:"Neural Networks",id:"neural-networks",level:2},{value:"Overfitting",id:"overfitting",level:3},{value:"Training and generalisation error",id:"training-and-generalisation-error",level:3}],p={toc:u};function m(e){let{components:t,...r}=e;return(0,a.kt)("wrapper",(0,n.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"text-classification"},"Text classification"),(0,a.kt)("h2",{id:"classification-as-generalisation"},"Classification as generalisation"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Core assumption: our data is representative of the real world (sampled from the same distribution). You are never observing the real world, only a sample of it"),(0,a.kt)("li",{parentName:"ul"},"e.g. Language competency classifier trained in this class")),(0,a.kt)("h2",{id:"arising-issues"},"Arising Issues"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Geographical difference, language"),(0,a.kt)("li",{parentName:"ul"},"Cultural, words changing meaning based on dominant culture, youth slang"),(0,a.kt)("li",{parentName:"ul"},"Time difference, change in vocabulary, concept drift")),(0,a.kt)("h2",{id:"ml-is-generalisation"},"ML is generalisation"),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Machine learning")," - techniques that enable a computer to learn from examples\n",(0,a.kt)("strong",{parentName:"p"},"Deep learning")," - neural networks with multiple layers\n",(0,a.kt)("strong",{parentName:"p"},"Training a supervised model")," -  iteratively modifying parameters of the model to minimise training error in a way that (we think) leads to minimising generalisation error, which we estimate with testing error"),(0,a.kt)("h3",{id:"machine-learning-examples"},"Machine Learning Examples"),(0,a.kt)("p",null,"Traditional Machine Learning"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Linear models (Logistical Regression)"),(0,a.kt)("li",{parentName:"ul"},"Tree models (Decision Tree)"),(0,a.kt)("li",{parentName:"ul"},"Probabilistic models "),(0,a.kt)("li",{parentName:"ul"},"Similarity-based models"),(0,a.kt)("li",{parentName:"ul"},"Representation is explicitly defined by the user and fed to the model\nDeep Learning"),(0,a.kt)("li",{parentName:"ul"},"Neural network models"),(0,a.kt)("li",{parentName:"ul"},"Convolutional neural networks"),(0,a.kt)("li",{parentName:"ul"},"Recurrent neural networks"),(0,a.kt)("li",{parentName:"ul"},"Transformer models"),(0,a.kt)("li",{parentName:"ul"},"Representation is typically learnt by the model as part of the training process")),(0,a.kt)("h2",{id:"ml-approaches"},"ML Approaches"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Supervised")," - There is known ground truth about the data"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Unsupervised")," - There is no ground truth known about the data"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Semi-supervised")," - There is ground truth known about some of the data"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Reinforcement learning")," - We are trying to learn policies/sequences of action")),(0,a.kt)("h2",{id:"neural-networks"},"Neural Networks"),(0,a.kt)("p",null,"Also used in LLM"),(0,a.kt)("h3",{id:"overfitting"},"Overfitting"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Each additional hidden layer increases expressiveness",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Network can learn more complex concepts"),(0,a.kt)("li",{parentName:"ul"},"Training error gets smaller"))),(0,a.kt)("li",{parentName:"ul"},"Drawbacks:",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Data-hungry"),(0,a.kt)("li",{parentName:"ul"},"Inefficient"),(0,a.kt)("li",{parentName:"ul"},"Overfitting (memorises the training data)")))),(0,a.kt)("h3",{id:"training-and-generalisation-error"},"Training and generalisation error"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Minimise the training error"),(0,a.kt)("li",{parentName:"ul"},"Generalisation error - the one which matters. Cant be sure that training error matches generalisation error"),(0,a.kt)("li",{parentName:"ul"},"The process of finding the best trade-off between simplicity and training error is regularisation"),(0,a.kt)("li",{parentName:"ul"},"Want to find the best compromise between;",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Minimising training error"),(0,a.kt)("li",{parentName:"ul"},"Minimising model complexity"))),(0,a.kt)("li",{parentName:"ul"},"Simpler model is more robust")))}m.isMDXComponent=!0}}]);