# Properties of determinants
Determinant of a matrix $A$ is equal to the determinant of its transpose matrix

- **Transpose Matrix**: $detA = det A^T$
- **Triangular Matrix**: Equal to the product of the diagonal elements
- **Row(Column) swap**: If two rows (columns) are swapped the determinant of the matrix $A_s$ is $-detA$
- **Null determinant**: if two rows(columns) are identical(sum) then $detA=0$
	- If and only if the rows(columns) are [linearly dependent](../1043%20-%20Maths%202/22.02.09%20-%20Matrices.md#fundamental-property-of-linear-dependence)  
	- if and only if at least one row(column) is a linear combination of the other rows(columns)
	- if a row(column) is proportional to another row(column)
- **Invariant Determinant**: Row(column) the elements of another row(column) all multiplied by the same scalar are added, the determinant remains the same
- **Row(column) multiplied by a scalar**: Row(column) is multiplied by $\lambda$ then $\lambda detA$
- **Matrix multiplied by a scalar**: If $\lambda$ is scalar, $det(\lambda A) = \lambda ^n detA$
- **Determinant of the product**: Product between two matrices is equal to the products of the determinants. $det(AB) = det(A)det(B) = det(B)det(A) = det(BA)$

# Adjugate Matrices
## Submatrix
Obtained from $A$ by cancelling $m-r$ rows and $n-s$ columns. Where r,s 2 positive integers such that $1\le r \le m$ and $1\le s\le n$

- Rows dont have to be continuous
- $A=\begin{pmatrix} 3&3&1&0\\ 2&4&1&2\\ 5&1&1&1\\ \end{pmatrix}$
- Can be obtained by cancelling the second row, second and fourth column 
- $\begin{pmatrix} 3&1\\ 5&1 \\ \end{pmatrix}$

---
- **Minor**: Determinant of submatrix
- **Major**: If submatrix is the largest square
- **Complement submatrices**: Obtained by cancelling on the $i^{th}$ row and the $j^{th}$ column from $A$ to the element $a_{i,j}$
- **Complement Minor**: Determinant, indicated $M_{i,j}$
- **Cofactors**: $A_{i,j} = (-1)^{i+j}M_{i,j}$

## Adjugate Matrix
$A=\begin{pmatrix}
a_{1,1}&a_1,_2&...&a_1,_n \\
a_2,_1&a_2,_2&...&a_2,_n \\
...&...&...&...\\
a_n,_1&a_n,_2&...&a_n,_n \\
\end{pmatrix}$

Let $A_{i,j}$ be the cofactor for $a_{i,j}$. The *adjugate matrix* (adjunct or adjoin) $A$ is:
$adj(A)=\begin{pmatrix}
A_1,_1&A_2,_1&...&A_n,_1 \\
A_1,_2&a_2,_2&...&A_n,_2 \\
...&...&...&...\\
A_{1,n}&A_{2,n}&...&A_{n,n}\\
\end{pmatrix}$

Dan Terms: Transpose it  (Flip i,j around), then calculate the determinate of the remaining rows once removed it(the sub matrix, as would do normally). Then alternate between 1 and -1.(Use cofactors formula!. $(-1)^{i+j}$)

Tutorial Def:
1. $A$
2. $A^T$
3. All Compliment miners
4. Multiple by coefficent = adjudigate matrix
5. $A^{-1} = \frac{1}{detA}\times adj(A)$

# Laplace Theorems
## Theorem 1
Determinant of $A$ can be computed as the sum of each row(column) multiplied by the corresponding cofactor
$det A = \sum ^n _{j=1} a_{i,j} A_{i,j}$ for any arbitrary $i$ 
$det A = \sum ^n _{i=1} a_{i,j} A_{i,j}$ for any arbitrary $j$ 

The determinant of a one-element matrix is just the element $(det(a = a))$, can compute the determinant of any square number 

## Theorem 2
Sum of the elements of a row(column) multiplied by the corresponding cofactor related to another row(column) is always zero
$\sum ^n _{j=1} a_{i,j} A_{k,j}=0$ for any arbitrary $k\ne i$ 
$\sum ^n _{i=1} a_{i,j} A_{i,k}=0$ for any arbitrary $k\ne j$ 

minors are the row you remove, so you take sub matrix of the remaining part. $k$ = minor

# Introduction to Matrix inversion
## Inverting a matrix
For a square matrix, $A$, the inverse is $A^{-1}$ which is define as the matrix for which $AA^{-1}=I$
- **Invertible Matrices**: $\exists$ a matrix $B \in \R _{n,n} | AB=I=BA$
- **Unique Inverse** $\exists!$ a matrix $B \in \R _{n,n} | AB=I=BA$
- **Inverse matrix $A^{-1}$**: $A^{-1} = \frac{1}{detA} adj(A)$
- **Singular/Non Invertable/ Linear Dependent**: $det(A) =0$
- **Non-singular**: $det(A)\ne 0$
- **Inverse of a matrix product**: $(AB)^{-1}=B^{-1}A^{-1}$