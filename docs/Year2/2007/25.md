# 25. Revision

## Computer Design
### Registers
- **Data Registers** - Any sort of data for special functions. 
- **Program Counter-** Holds the next instruction.
- **Program status word**  - admin, stores the mode bit. Which mode the CPU is in, kernel mode or user mode.
- **User Mode** - Direct access to a subset of instructions the CPU can carry out
- **Kernel mode** - Access to the full set of instructions. Including privileged memory locations. 
- Normally compiler decides which mode to use

### Memory Management Unit
*Location of an address*
Doesn't know where in memory an executable will run. Variables require memory, they need to have a memory space
- **Logic Address Space** -  Used by the processor and the compiler (starts at 0). What you use when you write code. Every process has one [0,MAX^64] 
- **Physical Address Space** - Seen by the hardware/OS [0,MAX] (Determined by the amount of physical memory)
Also responsible for address translation `physical = locgical + x`

### Moors Law
> **Moore's law** - The number of transistors on an integrated circuit doubles roughly every two years. 

Closely linked but necessarily related to performance. 
Still continuing but the power wall slows performance improvements of single core
Can extract parallelism automatically, can implement it at the lowest level

### Multi-core, hyper-threaded processors
Evolution in hardware has implications on OS design
The process scheduling needs to account for load balancing and CPU affinity. Need to decide **when** and **where** it is going to run

### Micro Kernels
All non-essential functionality is extracted from the kernel. These are easier to extend, more portable and more reliable.
Frequent system calls cause mode switches/overhead

**Monolithic Systems** - All procedures into one single executable running in kernel mode. However, they are difficult to maintain. Current versions of Windows, and Linux are implemented as this.

## Introduction to processes
Running instance of a program. This has 'control structures' - they store all the necessary information related to the management of the process

### Context Switching
Process control box contains:
- **Process identification** (PID, UID, Parent PID)
- **Process control information** (process state, scheduling information)
- **Process state information** (user registers, program counter, stack pointer, program status word, memory management information, files)
The **process control block** is **necessary** for **context switching** in **multi-programmed systems**
Process control blocks are kernel data structures, which are stored in the **process table** and are accessible in the **kernel mode** only (system calls), otherwise this would compromise their integrity.

#### Multi-Programming
Achieved by alternating processes and context switching
- Single processor systems results in concurrent execution
- True parallelism requires multiple processors

Slow time slices - good response times, low utilisation
Long time slices - poor response times, better utilisation

#### Process
Memory image contains: the program (shared) code, data segment (stack/heap)
Every process has own logical address space
Some OS address space layout are randomised 

### Process implementation
Information about the status of "resources" is maintained in tables. These are in kernel space and cross referenced
- **Process tables** - process control blocks
- **Memory tables** - memory allocation, protection, virtual memory 
- **I/O tables** - Availability, status, transfer information
- **File tables** - Location, status

### States and Transitions
![](../_resources/20221007111037.png)
- **New** process has just been created and is waiting to be admitted
- **Ready** process is waiting for the CPU to become available
- **Running** process 'owns' the CPU
- **Blocked** process cannot continue (waiting for IO)
- **Terminated** process is no longer executable
- **Suspended** process is swapped out

### System Calls
- True system calls are "wrapped" in the OS libraries following a well defined interface
- These are necessary to notify the OS that the process has terminated
- `fork()` - creates an exact copy of the current process

## Process Scheduling
- New -> ready: when to admit processes to the system
- Ready -> running: decide which process to run next
- Running -> ready: when to interrupt process
- **Scheduler** - decides which process to run next
- **Type of operating system** - determines which algorithms are appropriate

### Time Horizon
- **Long term** - admits new processes and controls the degree of multi programming. Good mix of CPU and IO bound processes. Usually absent in popular modern 
- **Medium term** - controls swapping. Looks to see how busy the system currently is. 
- **Short term** - which process to run next. Manages the ready queue, runs frequently (must be fast). Called following clock interrupts or blocking system calls.

### Process Schedulers
- **Non-Preemptive** - Processes are interrupted voluntarily
- **Preemptive** - Processes are interrupted forcefully or voluntarily. Requires context switches. Prevents process from monopolising the CPU. Most popular OS are preemptive

### Performance Assessment
- User Oriented Criteria
	- **Response time**: time between creating the job and its first execution
	- **Turnaround time**: time between creating the job and finishing it
	- **Predictability**: variance in processing times
- System oriented criteria:
	- Throughput: number of jobs processed per hour
	- Fairness: Equally distributed processing.

### Scheduling Algorithms
| Algorithm                                  | Concept                                                                                                                                                                | Advantage                                                                              | Disadvantage                                                                                                      |
| ------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |
| First Come First Served/First in First out | Non-preemptive algorithm that operates as a strict queuing mechanism.                                                                                                  | Positional fairness an easy to implement                                               | Favours long processes over short ones. Could compromise resource utilisation.                                    |
| Shortest Job First                         | A non-preemptive algorithm that starts processes in order of ascending processing time                                                                                 | Always result in the optimal turnaround time                                           | Starvation might occur. Fairness and predictability are compromised. Processing times have to be known beforehand |
| Round Robin                                | A preemptive version of FCFS. Processes run in the order they were added but they only get a max amount of time at once. Forces context switches at periodic intervals | Improved response time. Effective for general purpose interactive/time sharing systems | Increased context switching and overhead. Favours CPU bound processes over IO. Can reduce to FCFS.                |
| Priority Queue                             | A preemptive algorithm that schedules processes by priority. Round robin is used within the same priority levels. Saved by the process control block                   | Can priorities IO bound jobs                                                           | Low priority may suffer from starvation                                                                           |


## Threads
- **Resources** - All related resources are grouped together
- **Execution trace** - entity that gets executed
- A process can share its resources between multiple execution traces/threads

Every thread has its own execution context & thread control block, however they all have access to the processes shared resources.
Execution of a process has states (new, running, blocked, read, terminated)

| Shared Resources (Processes) | Private Resources (Threads) |
| ---------------------------- | --------------------------- |
| Address space                | Program Counter             |
| Global variables             | Registers                   |
| Open files                   | Stack                       |
| Child processes              | State                       |
| Pending alarms               | Local vars                  |
| Signals and signal handling  |                             |
| Accounting information       |                             |

Threads incur less overhead to create/terminate/switch
Hyper threaded cores have hardware support for multi-threading

- **Inter-thread communication**: Easier/faster than **interprocess** communication (memory is shared)
- **No protection boundaries**: Required in the address space (threads are cooperating, belong to the same user, and have a common goal)
- **Synchronisation**: Has to be considered carefully

### Reason to use threads
- Processes will often contain multiple blocking tasks (IO operations & memory access page faults)
- Some activities should be carried out in parallel/concurrently such as webservers, msoffice etc

### User Threads
*Many-to-One*
- Thread management is executed in user space with the help of a user library
- Process maintains a thread table, managed by the runtime system without the kernels knowledge.
- Kernel can see 1 process, but user space can see multiple
- Advantages - Full control over the thread scheduler, OS independent and in user space
- Disadvantages - Blocking system calls suspend the entire process (can be cause by page faults), no true parallelism, clock interrupts don't exist.

### Kernel Threads
*One-to-One*
- Kernel manages threads, user access them through system calls. Thread table is maintained by the kernel. If a thread blocks, the kernel chooses thread
- Advantages - True parallelism, no run-time needed
- Frequent mode switches take place (performance)

### Performance
- **Null fork** - the overhead is creating, scheduling, running and terminating a null process/thread
- **Signal wait** - overhead is synchronising threads

### Hybrid Implementations
- User threads are multiplexed onto kernel threads
- Kernel sees and schedules the kernel threads
- User applications sees user threads and creates/schedules these

### Thread Management
- Libraries are either user space or based on system calls

## Process Scheduling
Characteristics of feedback queues include; number of queues, scheduling algorithms, migration policy and initial access. These queues are highly configurable.
- **Real time** - processes/threads have a fixed priority level
- **Variable** - processes/threads can have their priorities boosted temporarily 
Priorities are based on the process base priority(0-15) and thread base priority (+-2 relative to the process priority)
Threads priority dynamically changes during execution. (between base and max priority)

### Completely fair scheduler
Linux has Real time tasks, FIFO and Round Robin, and Time sharing tasks using a preemptive approach.
Real time FIFO takes highest priority, and preemption if a higher priority shows up
Real time round robin tasks are preemptable by clock interrupts and have a time slice associated with them. 

#### Equal priority
CFS divides the CPU time between all processes/threads. If all *N* have same priority, they will be allocated a time slice. The length of the time slice are based on the targeted latency. If *N* is very large, the context switch time will be dominant, and a lower bound on the time slice.

#### Different priority
A weighing scheme is used to take different priorities into account. If different priority,  then every *i* is allocated a weight *w* that reflects its priority. The tasks with the lowest proportional amount of used CPU time are selected first

### Multi-processor scheduling
- **Single Processor** machine - Which process(thread) to run next
- **Shared Queues** - Single or multi-level shared between all CPUs. + Automatic load balancing. - Contention for the queues (locking if needed), does not account for processor affinity
- **Private Queues** - Each CPU has a private set of queues. + CPU affinity is automatically satisfied, contention for shared queues minimised. - Less load balancing, push and pull migration between CPUs is required
- **Related Threads** - Multiple threads that communicate with one another and ideally run together (search algorithm)
- **Unrelated Threads** - Processes threads that are independent, possibly started by different users running different programs
- Working together - Aim is to get threads running, as much as possible, at same time across multiple CPUs
- **Space Scheduling** - Number *N* can be dynamically adjusted to match processor capacity
- **Gang Scheduling** - Time slices are synchronised and the scheduler groups threads together to run simultaneously. Preemptive algorithm, blocking threads result in idle CPUs

------------
*This is where Dan the retard compresses his notes instead of 'shortening' them*

## Concurrency
- Threads/processes execute concurrently and share resources. These can be interrupted at any point. Process state is saved in the process control block.
- Outcomes of programs are unpredictable. Sharing data leads to inconsistencies. 

### Race Conditions
- Occurs when multiple threads/processes access shared data and same time.
- Mechanisms such as semaphores and mutexes can prevent this.
- OS must make sure that interactions within the OS do not result in race conditions
	- Must provide locking mechanisms to implement/support mutual exclusions and prevent starvation/deadlocks

**Critical Section** - Set of instructions in which shared resources between processes/threads are changed. Only one can access it at once, so need to ensure it gets locked whilst in use. Solutions must allow process to enter it as some point, and ensure there is fairness within the program.

**Mutual Exclusion** - Must be enforced for critical sections. Processes need to get permissions before entering critical section. Solutions can be software based (Peterson solution), hardware, mutexes/semaphores, monitors. Deadlocks also need to be prevented

### Deadlocks 
> Set of p/t is deadlocked if each p/t in the set is waiting for an event that only the other p/t in the set can access

Mutexes can cause deadlocks. **All** four conditions must hold for deadlocks to occur
1. **Mutual exclusion**: a resource can be assigned to at most one process at a time
2. **Hold and wait condition**: a resource can be held whilst requesting new resources
3. **No preemption**: resources cannot be forcefully taken away from a process
4. **Circular wait**: there is a circular chain of two or more processes, waiting for a resource held by the other processes.

### Peterson's Solution
- **Software Solution** - Worked well on older machines. Two shared variables are used `turn` (next in critical section) and `boolean flag[2]`(process is read to enter the critical section.[Code](07.md#Peterson's_Solution)
- **Mutual Exclusion Requirement** - Satisfies all critical section requirements. Only want one process on the thread accessing the critical section at once
- **Progress Requirement** - Any process must be able to enter its critical section at some point in time.
- **Fairness/bound waiting** - Fairly distributed waiting times/processes cannot be made to wait indefinitely

**Disabling Interrupts** - Disable whilst executing a critical section and prevent interrupts. May be appropriate on single CPU machine. Insufficient on modern machines

**Atomic Instructions** - Implement `test_and_set()` and `swap_and_compare()` instructions as a set of atomic (= uninterruptible) instructions. Reading and setting is done as one complete instruction. If called simultaneously, they will be executed sequentially.

**Mutual Exclusion** - `test_and_set()` and `swap_and_compare()` are hardware instructions and not directly accessible to the user. Other disadvantages include busy waiting, and deadlocks

### Mutexs & Semaphores
#### Mutexes
- Mutexes are an approach for mutual exclusion provided by the operating system. Contains boolean lock variable to indicate. Lock is set to **true** if **available**. **false** if **unavailable**
- Two functions are used, must be atomic instructions;
	- `acquire()` - called before a critical selection, sets bool to false. Results in busy wait and bad for performance on single CPU systems
	- `release()` - called after the critical section, sets bool to true
- Advantages - Context switches can be avoided, efficient on multi-core/processor system.
- Can be implemented as sempahore

#### Sempahores
- Does the opposite, puts the process on the execution and puts it to sleep
- Contains an integer variable, well distinguished between binary and counting sempahores, force mutual exclusion. Positive values indicate its available, negative means how many are waiting
- Two atomic functions
	- `wait()` - called when a resource is acquired, the counter is decremented
	- `signal()`/`post()` - is called when a resource is released
- Different queuing strategies can be employed to remove p/t, queues as FIFO


**Efficiency** - Performance penalty, only synchronise whats necessary and as few as possible

**Caveats** 
- Starvation - Poorly designed queuing approaches (LIFO)
- Deadlocks - Every thread in a set is waiting
- Priority Inversion - High priority process waits for a resources held by a low priority process. This can happen in chains, prevented by inheritance/boosting

### The Producer/Consumer Problem
- Producers and consumers share *N* buffers that are capable of holding one item each.
- Producer(s) add items and goes to sleep if buffer is full
- Consumer(s) removes items and goes to sleep if the buffer is empty

#### First Version
- 1 of each, unbounded buffer. Counter(index) represents the number of items in the buffer
- Solution consists of two binary semaphores
	- `sync`: synchronises access to the buffer (counter) - initialised to 1
	- `delay_consumer`: puts the consumer to sleep when the buffer is empty - initialised to 0

![](../_resources/20221021180655.png)
**Race condition** - When the consumer has exhausted the buffer, should have gone to sleep, but the producer increments `items` before the consumer checks it

#### Second Version
Other variant of the problem has *n* consumers, *m* producers, and a fixed buffer size *N*. Solution is based on 3 semaphores
- `sync` - enforce the mutual exclusion for the buffer
- `empty` - keeps track of the number of empty buffers, initialised to *N*
- `full` - keeps track of the number of full buffers, initialised to 0

### The Dining Philosophers Problem
- 5 Philosophers sitting around table, and require two forks, but there are only 5.
- When hungry (in between thinking) they try to acquire the fork on his left and right, these are represented by semaphores
	- 1 - fork is available
	- 0 - fork is not available

#### Solution 1 - Deadlock
- Every philosopher picks up one fork and waits for the second one to become available (without putting the first one down)
- Fixed this by having random wait time, or an additional fork.

#### Solution 2 - Global Mutex/Semaphore
- Could have global mutex when they want to eat, but this results into only one being able to eat

#### Solution 3 - Maximum Parallelism 
- `state[n]` - one state variable for every philosopher
- `phil[n]` - one semaphore per philosopher. (Sleeps if neighbour is eating, wakes up once finished eating)
- `sync` - one semaphore/mutex to enforce mutual exclusion of the section (updating states)
- [Code](10.md#solution_3_-_maximum_parallelism)

### Reader/Writers Problem
- Concurrent database processes are readers/writers, files etc
- Reading a record can happen in parallel, writing needs synchronisation
- Different solutions, naive implementation (one at a time) (limited parallelism), readers receive priority, writing is performed asap

## Memory Management
OS Responsibilities - (de)allocate memory between process when required and keep track. Control access when multi-programming is applied

### Partitioning
- **Contiguous memory management models** - allocate memory in one single block without any holes or gaps
- **Non-contiguous memory management models** - capable of allocating memory in multiple blocks, or segments, which may be placed anywhere in physical memory
#### Contiguous Approaches
- **Mono-programming** - one single partition for user processes
- **Multi-programming** with fixed (non)equal partitions
- **Multi-programming** with dynamic partitions

### Mono-Programming
- One process at a time, and a fixed region of memory is allocated to the OS, remaining memory is reserved for a single process.
- Process has direct access to its allocated contiguous block of memory
- One process is allocated the entire memory space, and the process is always located in the same address space. Overlays allow the program to control the memory.
- Shortcomings
	- Direct access to the physical memory means it has access to OS memory.
	- OS can be seen as a process, and low utilisation of hardware resources
- Simulating - Multi-programming through swap process out to the disk and load a new one, apply threads within the same process

### Multi-Programming
$n$ - process in memory
$p$ - processor spends percent of its time waiting for I/O
$1-p$ - CPU utilisation 
$p^2$ - Probability that all $n$ processes are waiting for I/O

![](../_resources/20221031165944.png)
This model assumes that all processes are independent, not true
More complex models could be build queuing theory, can still use simplistic models to make predictions 

## Partitioning 
**Fixed partitions of equal size** - Divide memory into static, contiguous, and equal sized partitions that have fixed size and location.
- Disadvantages - Low memory utilisation and internal fragmentation: partition may be unnecessarily large. Overlays must be used if a program does not fit into a partition

**Fixed partitions of non-equal size** - Divide memory into static, non-equal sized partitions that have a fixed size and location. This reduces internal fragmentation, but the allocation of process to to partitions must be carefully considered

**Fixed Partitions (Allocation Methods)** - One private queue per partition. Assigns each process to the smallest partition that it would fit in. Reduces internal fragmentation and can reduce memory utilisation and result in starvation.

A single shared queue for all partitions can allocate small process to large partitions but result in increased internal fragmentation



