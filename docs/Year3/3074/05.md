# Representation and Similarity

## Fixed size representation - Bag of words model
- NLP algorithms don't understand words
- Several algorithms work with fixed-size vectors
- Relies on comparing documents

Those documents need to be represented in a common space
A vector space is a natural fit to represent this space
- Each word/term is a dimension of that vector space
- Each document is a vector in that vector space

### Distance and Similarity in Vector Space
Similarity measures
- At the core of many algorithms
	- Information retrieval algorithms (search by similarity)
	- Clustering algorithms (cluster by similarity)
	- Some classifiers, recommender systems, etc
- Intrinsically related to distance measures

### Similarity in vector spaces
- Jaccard index - Works with binary weights
- Euclidean distance - Standard geometry distance
- Cosine similarity - Angle of the document vectors. Dot product of those vectors divided by the multiplication of their norms

### Representation
- Binary term weighting - ....
- Log-frequency term weighting
- TF-IDF(Term Frequency - Inverse Document Frequency) term weighting - Modification of the log frequency. Adding 
- There are more advanced and specialised representations

### Applications
- Can be fed into ML algorithms (classifiers, clustering etc)
- Can be used for search engines. Gives us the notion of distance and similarity

## Variable-length representations - One hot encoding, and word embeddings

### One-hot encoding
- Assumptions: all words are equally unrelated
- No semantics
- Large sparse matrices (mostly 0s)

### Word Embeddings
- Semantics - the study of meaning
- Distributional semantics - modelling meaning based on the distribution of language in large samples of data
- A word embedding is a dense vector representation of words
- Tend to preserve some properties of words
...
Larger the NN the more expensive
....

## Applications of similarity