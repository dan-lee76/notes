# Modelling, representing, & generating language

### Outline and objectives
- Modelling and generation are big topics

Language **modelling** - Building a probabilistic model of language. 
Language **generation** - Using templates and probabilistic models of language to create natural-sounding testing
## Language Modelling
### Probabilistic Language Models
- Core problem of language modelling
- Given a sequence of tokens, predict the next token

Difficult to predict what word is next/accurate, is it 'I like feeding pigeons' or 'I like feeding bludgeons'. It is difficult to observe as what if that sentence doesn't appear in our data set.

- Each word `i` depends only on a **restricted set of preceding words**
- Assumption: Language is a **stochastic**, **memoryless process**


We only care about the probabilities of individual words

### Maximum Likelihood Estimation and its problems
... is the Maximum Likelihood Estimate (MLE)


### Smoothing
- What if we apply our language model on a new sentence
	- One novel bigram would pull the probability to 0


### Conclusion
- Smallest unit of thought is the n-gram
	- Unigrams let us work with single words
	- Bigrams let us work with pairs of consecutive words
	- Trigrams let us work with triples of consecutive words
- Language models let us compare the likelihood of documents

## Language Generation
### What is NLG
- NLG is the subfield of ai and computational linguistics that focuses on computer sytems that can **produce understandable texts**
- ...

### Multiple schools of NLG
- Gap-filling systems - Simplest, no novelty
- Rule-based systems - Expanded gap-filling system
- Grammatical function - Complex template system, low novelty, medium error rate
- Dynamic generation - Abstract representation, fully dynamic, high novelty, high error rate

NLP = Natural Language Understanding + Natural Language Generation